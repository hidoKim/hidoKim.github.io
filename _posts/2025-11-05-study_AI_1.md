---
layout: single
title: "[AI] 트랜스포머 아키텍처 기반 모델"
categories: AI_study
tags: [AI_study, study]
toc: true
author_profile: false
---

# 트랜스포머 아키텍처 기반 모델

---

# ✅ 트랜스포머 아키텍처란?

- 트랜스포머(Transformer)는 Attention(어텐션) 메커니즘에 기반해 입력 시퀀스의 모든 요소 간 관계를 효과적으로 파악하고 처리하는 딥러닝 모델 구조.

- 기존 RNN/LSTM 기반 seq2seq의 한계를 극복하고, 병렬 처리와 긴 문맥 포착이 뛰어남.

- 대표적으로 자연어 처리(NLP) 분야에서 광범위하게 사용되고, 번역, 요약, 질의응답, 텍스트 생성 등 다양한 Task에 적용됨.

## 1️⃣ 트랜스포머 아키텍처의 구조

### 인코더 기반 📥

입력 전체를 한 번에 받아 각 토큰의 상호관계를 양방향으로(양쪽 Context) 인코딩함.

- 대표 모델: BERT, RoBERTa, KoBERT(한국어).​

- 주요 Task: 문장 분류, 감정 분석, 개체명 인식, 문맥 이해 등 **분석** 중심 작업.​

### 디코더 기반 📤

이전까지의 생성된 출력만을 바탕으로 순차적으로 다음 토큰을 예측·생성함.

- 대표 모델: GPT, KoGPT(한국어).​

- 주요 Task: 텍스트 생성, 대화, 요약 등 **생성** 중심 작업.​

### 인코더-디코더 기반 📥 📤

인코더가 입력 정보를 벡터로 임베딩하면, 디코더가 이를 바탕으로 출력 시퀀스를 순차적으로 생성함.

- 대표 모델: T5, BART, KoT5(한국어).​

- 주요 Task: 기계 번역, 요약, 질의응답 등 입력과 출력이 모두 시퀀스인 작업에 강점.​

### 구조별 특징 및 대표 모델 비교

| 구조               | 대표 모델 | 텍스트 지원 | 이미지 지원                     | 모델명/제공사      | 연도 | 한국어 지원         | 주요 Task               | 모델 크기(파라미터 수)    | 특징/설명                                    |
| ------------------ | --------- | ----------- | ------------------------------- | ------------------ | ---- | ------------------- | ----------------------- | ------------------------- | -------------------------------------------- |
| 인코더 기반        | BERT      | O           | X                               | BERT/구글          | 2018 | O                   | 분류, QA, NER 등        | Base: 1.1억, Large: 3.4억 | 양방향 문맥 이해, 텍스트 분석에 강점         |
|                    | KoBERT    | O           | X                               | SK텔레콤           | 2019 | O                   | 분류, QA, NER 등        | 1.1억                     | BERT 구조, 한국어 데이터로 사전학습          |
| 디코더 기반        | GPT       | O           | (GPT-4 이상 O, GPT-3 X)         | GPT/OpenAI         | 2018 | △ (GPT-3 일부 지원) | 텍스트 생성, 요약, 대화 | GPT-3: 1,750억            | 단방향, 문장 생성·자동 완성 등 생성에 특화   |
|                    | KoGPT     | O           | X                               | Kakao Brain        | 2021 | O                   | 생성, 대화, 요약        | KoGPT-Base: 1.25억        | GPT-2 구조, 한국어 특화                      |
| 인코더-디코더 기반 | T5        | O           | O (텍스트→이미지 보조 임베딩용) | Google             | 2019 | O                   | 번역, 요약, QA, 생성    | Base: 2.2억, Large: 11억  | 모든 태스크를 텍스트-투-텍스트 형식으로 통합 |
|                    | KoT5      | O           | X                               | Kakao Brain        | 2021 | O                   | 번역, 요약 등           | Base: 1.8억               | T5 구조, 한국어 사전학습 모델                |
|                    | BART      | O           | X                               | Meta (Facebook AI) | 2019 | 일부 지원           | 요약, 생성, 복원        | Base: 1.4억, Large: 4억   | 인코더-디코더, 요약·생성에 강점              |

## 2️⃣ 트랜스포머 아키텍처의 특징

- RNN과 달리 입력 전체에 대한 병렬 연산이 가능해 학습/추론 속도가 빠름.​

- 어텐션 메커니즘으로 문장 내 장거리 의존성을 잘 처리함.​

- 다양한 프리트레인 모델이 공개되어 모델 선택 및 활용이 쉽고, 사전훈련 후 파인튜닝으로 다양한 Task에 적용 가능.​

- 아키텍처별 한국어 특화 모델 참고: KoBERT(분류), KoGPT(생성), KoT5(번역/요약 등)

# 출처

[koBERT](https://wikidocs.net/229878)

[Ko-GPT](https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5)

[T5](https://velog.io/@yunseo4401/CS224N-Lecture14-T5-and-Large-Language-Models)

[BART](https://wikidocs.net/225013)
